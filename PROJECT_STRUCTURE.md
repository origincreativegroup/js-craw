# Project Structure

This document describes the comprehensive project and implementation structure for the Job Search Crawler system.

## ğŸ“ Directory Structure

```
js-craw/
â”œâ”€â”€ app/                          # Main application package
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ config.py                 # Configuration & settings (Pydantic)
â”‚   â”œâ”€â”€ database.py               # Database connection & session management
â”‚   â”œâ”€â”€ models.py                 # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ api.py                     # FastAPI routes & endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ crawler/                   # Job platform crawlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ linkedin_crawler.py    # LinkedIn automation (Selenium)
â”‚   â”‚   â”œâ”€â”€ indeed_crawler.py      # Indeed automation (Selenium)
â”‚   â”‚   â””â”€â”€ orchestrator.py        # Coordinates multi-platform crawling
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/                        # AI analysis modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ analyzer.py           # Ollama LLM integration for job analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ notifications/             # Notification services
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ notifier.py            # Push notifications (ntfy/Pushover/Telegram)
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utility modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ crypto.py              # Password encryption (Fernet)
â”‚
â”œâ”€â”€ static/                        # Frontend files
â”‚   â””â”€â”€ index.html                 # Web dashboard (single-page app)
â”‚
â”œâ”€â”€ scripts/                       # Utility scripts
â”‚   â””â”€â”€ diagnose.sh                # Diagnostic and troubleshooting script
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md            # Technical architecture & design
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md         # Complete feature overview
â”‚   â””â”€â”€ SETUP.md                   # Detailed setup guide
â”‚
â”œâ”€â”€ tests/                         # Test files (future)
â”‚
â”œâ”€â”€ main.py                        # Application entry point (FastAPI)
â”œâ”€â”€ Dockerfile                     # Docker image definition
â”œâ”€â”€ docker-compose.yml             # Docker services orchestration
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ start.sh                       # Quick start script
â”œâ”€â”€ README.md                      # Main documentation
â””â”€â”€ PROJECT_STRUCTURE.md           # This file
```

## ğŸ—ï¸ Architecture Overview

### Application Layers

1. **API Layer** (`app/api.py`)
   - FastAPI REST endpoints
   - Request/response validation (Pydantic)
   - Database session management

2. **Business Logic Layer**
   - **Orchestrator** (`app/crawler/orchestrator.py`): Coordinates crawling workflow
   - **Crawlers** (`app/crawler/`): Platform-specific job extraction
   - **AI Analyzer** (`app/ai/analyzer.py`): Job matching and analysis
   - **Notifier** (`app/notifications/notifier.py`): Push notifications

3. **Data Layer**
   - **Models** (`app/models.py`): SQLAlchemy ORM models
   - **Database** (`app/database.py`): Connection pooling and session management

4. **Infrastructure Layer**
   - **Config** (`app/config.py`): Environment-based configuration
   - **Crypto** (`app/utils/crypto.py`): Credential encryption

## ğŸ”„ Data Flow

```
User Action / Scheduled Trigger
    â†“
FastAPI Route (app/api.py)
    â†“
CrawlerOrchestrator (app/crawler/orchestrator.py)
    â”œâ”€â†’ Get active searches from DB
    â”œâ”€â†’ Get encrypted credentials
    â””â”€â†’ Spawn platform crawlers
        â†“
    LinkedInCrawler / IndeedCrawler
        â”œâ”€â†’ Login with Selenium
        â”œâ”€â†’ Execute search
        â”œâ”€â†’ Parse job listings
        â””â”€â†’ Return raw job data
            â†“
    JobAnalyzer (app/ai/analyzer.py)
        â”œâ”€â†’ Call Ollama API
        â”œâ”€â†’ Generate summary, pros/cons
        â”œâ”€â†’ Calculate match score
        â””â”€â†’ Return analysis
            â†“
    Database (app/models.py)
        â”œâ”€â†’ Check for duplicates
        â”œâ”€â†’ Save new jobs with analysis
        â””â”€â†’ Update metadata
            â†“
    NotificationService (app/notifications/notifier.py)
        â”œâ”€â†’ Aggregate new jobs
        â”œâ”€â†’ Format message
        â””â”€â†’ Send to phone
```

## ğŸ“¦ Component Details

### Core Components

#### `main.py`
- FastAPI application initialization
- Lifespan management (startup/shutdown)
- APScheduler integration
- CORS configuration
- Static file serving

#### `app/config.py`
- Pydantic Settings for environment variables
- Type-safe configuration
- Default values for development

#### `app/database.py`
- Async SQLAlchemy engine
- Session factory
- Base model class
- Connection pooling

#### `app/models.py`
- **User**: Platform credentials (encrypted)
- **SearchCriteria**: Job search parameters
- **Job**: Job postings with AI analysis
- **FollowUp**: Reminder scheduling
- **CrawlLog**: Execution history

### Crawler Components

#### `app/crawler/orchestrator.py`
- Coordinates multiple platform crawlers
- Manages database transactions
- Handles AI analysis
- Sends notifications
- Logs crawl execution

#### `app/crawler/linkedin_crawler.py`
- Selenium-based LinkedIn automation
- Login handling
- Job search execution
- Job listing extraction

#### `app/crawler/indeed_crawler.py`
- Selenium-based Indeed automation
- No login required (optional)
- Job search execution
- Job listing extraction

### AI Components

#### `app/ai/analyzer.py`
- Ollama API integration
- Prompt engineering for job analysis
- JSON response parsing
- Match score calculation
- Report generation

### Notification Components

#### `app/notifications/notifier.py`
- Multi-platform support (ntfy/Pushover/Telegram)
- Message formatting
- Priority handling
- Error handling

### Utility Components

#### `app/utils/crypto.py`
- Fernet symmetric encryption
- PBKDF2 key derivation
- Password encryption/decryption

## ğŸ³ Docker Services

### Services in `docker-compose.yml`

1. **postgres**: PostgreSQL database
2. **redis**: Redis cache (for future use)
3. **selenium-chrome**: Selenium Grid Chrome node
4. **ollama**: Local LLM server
5. **job-crawler**: Main application

### Service Dependencies

```
job-crawler
    â”œâ”€â†’ postgres (database)
    â”œâ”€â†’ redis (cache)
    â”œâ”€â†’ selenium-chrome (browser automation)
    â””â”€â†’ ollama (AI analysis)
```

## ğŸ”Œ API Endpoints

### Search Management
- `GET /api/searches` - List all searches
- `POST /api/searches` - Create new search
- `PATCH /api/searches/{id}` - Update search
- `DELETE /api/searches/{id}` - Delete search

### Job Management
- `GET /api/jobs` - List jobs (with filters)
- `GET /api/jobs/{id}` - Get job details
- `PATCH /api/jobs/{id}` - Update job status

### Follow-ups
- `GET /api/followups` - List follow-ups
- `POST /api/followups` - Create follow-up

### System
- `POST /api/crawl/run` - Trigger manual crawl
- `GET /api/stats` - Dashboard statistics
- `POST /api/credentials` - Save platform credentials

## ğŸ“Š Database Schema

### Tables

1. **users**: Platform credentials (encrypted)
2. **search_criteria**: Job search parameters
3. **jobs**: Job postings with AI analysis
4. **follow_ups**: Reminder scheduling
5. **crawl_logs**: Execution history

### Relationships

```
User (1) â”€â”€< (N) SearchCriteria
SearchCriteria (1) â”€â”€< (N) Job
SearchCriteria (1) â”€â”€< (N) CrawlLog
Job (1) â”€â”€< (N) FollowUp
```

## ğŸ” Security Features

1. **Credential Encryption**: Fernet (AES-128) with PBKDF2 key derivation
2. **Local AI Processing**: 100% local with Ollama
3. **Encrypted Transit**: HTTPS for notifications
4. **Environment Variables**: Sensitive data in .env (not in code)
5. **Database Authentication**: PostgreSQL with credentials

## ğŸš€ Deployment

### Local Development

1. Copy `.env.example` to `.env`
2. Configure environment variables
3. Run `./start.sh` or `docker-compose up -d`
4. Access dashboard at http://localhost:8001/static/index.html

### Production Considerations

1. Change `SECRET_KEY` to random string
2. Set `DEBUG=false`
3. Configure proper database credentials
4. Set up reverse proxy (nginx/Caddy)
5. Add authentication to API
6. Use managed database (if needed)
7. Set up monitoring and logging

## ğŸ“ Development Guidelines

### Code Organization

- **Separation of Concerns**: Each module has a single responsibility
- **Async/Await**: All I/O operations use async patterns
- **Type Hints**: Full type annotations for better IDE support
- **Error Handling**: Comprehensive try/except blocks with logging
- **Logging**: Structured logging throughout

### Adding New Features

1. **New Crawler**: Add to `app/crawler/` following existing patterns
2. **New Notification Method**: Extend `app/notifications/notifier.py`
3. **New API Endpoint**: Add to `app/api.py`
4. **New Model**: Add to `app/models.py` and create migration

### Testing

- Unit tests: `tests/unit/`
- Integration tests: `tests/integration/`
- E2E tests: `tests/e2e/`

## ğŸ”„ Local Development Reference

This project structure follows patterns established in `origin/github/nexus.lan`:
- Clear separation of concerns
- Comprehensive documentation
- Scripts for automation
- Docker-based deployment
- Environment-based configuration

## ğŸ“š Additional Resources

- **Architecture Details**: See `docs/ARCHITECTURE.md`
- **Setup Guide**: See `docs/SETUP.md`
- **Feature Overview**: See `docs/PROJECT_SUMMARY.md`
- **API Documentation**: http://localhost:8001/docs (when running)

---

**Last Updated**: 2024
**Version**: 1.0.0

